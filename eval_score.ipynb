{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-26 21:20:48--  https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 0.0.0.0, ::\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|0.0.0.0|:443... failed: Connection refused.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|::|:443... failed: Connection refused.\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniXcoder(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(51416, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51416, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  re\n",
    "from torch import per_channel_affine\n",
    "from tqdm import tqdm\n",
    "import time, random\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import log\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取数据文件（先找个小一点的jsonl文件放到服务器上，选择CSN里ruby语言的test文件比较小）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained unixcoder tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n",
    "model = AutoModel.from_pretrained('microsoft/unixcoder-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279\n"
     ]
    }
   ],
   "source": [
    "#input:code snippet\n",
    "import jsonlines\n",
    "\n",
    "test_list=[]\n",
    "test_file=\"/home/huangyx/emscore/emscore/dataset/ruby_test_0.jsonl\"\n",
    "with open(test_file,mode='r') as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        test_list.append(item)\n",
    "print(len(test_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def build_sale_data(sale_item)\n",
      "        \"\".b.tap() do |data|\n",
      "          price_units = (sale_item.price * 100).to_i # !FIXME\n",
      "          price_bytes = \"\".b\n",
      "          4.times{ |shift| price_bytes.insert 0, ((price_units >> shift*8) & 0xff).chr }\n",
      "          data << price_bytes\n",
      "          qty_units = ((sale_item.qty || 1) * 1000).to_i # !FIXME\n",
      "          qty_bytes = \"\".b\n",
      "          4.times{ |shift| qty_bytes.insert 0, ((qty_units >> shift*8) & 0xff).chr }\n",
      "          data << qty_bytes\n",
      "          data << \"\\x00\".b #number len FIXME\n",
      "          data << \"\\xAA\\xAA\\xAA\\xAA\\xAA\\xAA\".b #number FIXME\n",
      "          text = sale_item.text1.truncate(20)\n",
      "          data << text.length.chr\n",
      "          data << text.ljust(20, \" \").b\n",
      "          data << (sale_item.tax_group || 2).chr\n",
      "        end\n",
      "      end\n",
      "----------------------------------------\n",
      "def build_sale_data(price, text1 = \"\", text2 = nil, tax_group = 2, qty = 1, percent = nil, neto = nil, number = nil)\n"
     ]
    }
   ],
   "source": [
    "#gain sample\n",
    "import json\n",
    "\n",
    "sample_list=random.sample(test_list,10)\n",
    "print(sample_list[0]['code'])\n",
    "print(\"----------------------------------------\")\n",
    "print(sample_list[0]['docstring'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到code和candidate的embedding向量,output_code/ouput_candidate就包含了每一个token/word向量\n",
    "#code_embeddings  candidate_embeddings就是分别对应整个向量表示，这里取的是mean操作获得\n",
    "\n",
    "\n",
    "\n",
    "# Encode input code and obtain word embeddings\n",
    "input_code = sample_list[0]['code']\n",
    "encoded_code = tokenizer.encode_plus(input_code, return_tensors='pt')\n",
    "output_code = model(**encoded_code)[0]\n",
    "code_embeddings = output_code.mean(dim=1)\n",
    "\n",
    "# Encode input candidate and obtain word embeddings\n",
    "input_candidate = sample_list[0]['docstring']\n",
    "encoded_candidate = tokenizer.encode_plus(input_candidate, return_tensors='pt')\n",
    "output_candidate = model(**encoded_candidate)[0]\n",
    "candidate_embeddings = output_candidate.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_summary(status)\n",
      "      status_string = status.to_s.humanize.upcase\n",
      "      if status == :success\n",
      "        heading(\"Result: \", status_string, :green)\n",
      "        level = :info\n",
      "      elsif status == :timed_out\n",
      "        heading(\"Result: \", status_string, :yellow)\n",
      "        level = :fatal\n",
      "      else\n",
      "        heading(\"Result: \", status_string, :red)\n",
      "        level = :fatal\n",
      "      end\n",
      "\n",
      "      if (actions_sentence = summary.actions_sentence.presence)\n",
      "        public_send(level, actions_sentence)\n",
      "        blank_line(level)\n",
      "      end\n",
      "\n",
      "      summary.paragraphs.each do |para|\n",
      "        msg_lines = para.split(\"\\n\")\n",
      "        msg_lines.each { |line| public_send(level, line) }\n",
      "        blank_line(level) unless para == summary.paragraphs.last\n",
      "      end\n",
      "    end\n",
      "-------------------\n",
      "Outputs the deferred summary information saved via @logger.summary.add_action and @logger.summary.add_paragraph\n"
     ]
    }
   ],
   "source": [
    "print(test_list[1]['code'] )\n",
    "print(\"-------------------\")\n",
    "print(test_list[1]['docstring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_code.shape)\n",
    "print(output_candidate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里的a参数指的是每一个candidate文本句子\n",
    "\n",
    "def process(a, tokenizer=None):\n",
    "    if tokenizer is not None:\n",
    "        a1 = tokenizer.encode(a, add_special_tokens=True, return_tensors=\"pt\").tolist()[0]\n",
    "    return set(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算词频，一共N个句子，统计单词在这些句子里出现几次，出现在一个句子里算一次，输入一个corpus里所有句子，输出所有单词对应的词频\n",
    "\n",
    "def get_idf_dict(arr, tokenizer, nthreads=4):\n",
    "    \"\"\"\n",
    "    Returns mapping from word piece index to its inverse document frequency.\n",
    "    Args:\n",
    "        - :param: `arr` (list of str) : sentences to process.\n",
    "        - :param: `tokenizer` : a BERT tokenizer corresponds to `model`.\n",
    "        - :param: `nthreads` (int) : number of CPU threads to use\n",
    "    \"\"\"\n",
    "    idf_count = Counter()\n",
    "    num_docs = len(arr)\n",
    "\n",
    "    process_partial = partial(process, tokenizer=tokenizer)\n",
    "\n",
    "    with Pool(nthreads) as p:\n",
    "        idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n",
    "\n",
    "    idf_dict = defaultdict(lambda: log((num_docs + 1) / (1)))\n",
    "    idf_dict.update({idx: log((num_docs + 1) / (c + 1)) for (idx, c) in idf_count.items()})\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_list=[]\n",
    "for item in test_list:\n",
    "    candidate_list.append(item['docstring'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算得到每个token对应的idf结果\n",
    "\n",
    "idf_dict=get_idf_dict(candidate_list,tokenizer)\n",
    "print(idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n",
    "model = AutoModel.from_pretrained('microsoft/unixcoder-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(vid_caps, model, tokenizer, idf_dict, device):\n",
    "    with torch.no_grad():\n",
    "        text_input = tokenizer.encode_plus(vid_caps, return_tensors='pt',return_attention_mask=True)\n",
    "\n",
    "        text_features = model(**text_input)[0]\n",
    "    #mask= text_input['attention_mask']\n",
    "\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # For special tokens, use [SOS] and [EOS]\n",
    "    txt_len = text_input['input_ids'].argmax(dim=-1)\n",
    "    mask = torch.zeros_like(text_input['input_ids'])\n",
    "    for i in range(len(mask)):\n",
    "        mask[i][0:txt_len[i]+1] = 1\n",
    "\n",
    "    idf_weights = torch.tensor([[idf_dict[int(i)] for i in a] for a in text_input['input_ids'].cpu()])\n",
    "\n",
    "    return text_features, mask, idf_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本处理之后的一些信息：\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[-0.0259, -0.0197, -0.0466,  ..., -0.0349,  0.0600,  0.0107],\n",
      "         [-0.0259, -0.0197, -0.0466,  ..., -0.0349,  0.0600,  0.0107],\n",
      "         [-0.0259, -0.0197, -0.0466,  ..., -0.0349,  0.0600,  0.0107],\n",
      "         ...,\n",
      "         [-0.0604,  0.0047, -0.0462,  ...,  0.0207, -0.0056,  0.0171],\n",
      "         [ 0.0185,  0.0171, -0.0471,  ..., -0.0024,  0.0075, -0.0214],\n",
      "         [-0.0312, -0.0250, -0.0111,  ...,  0.0002, -0.0119,  0.0200]]])\n",
      "torch.Size([1, 22, 768])\n",
      "关于mask的一些信息\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([1, 22])\n",
      "关于idf权重的一些信息：\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.0000, 4.5131, 6.3456, 1.4515, 4.8416, 6.3456, 2.8341, 0.8546, 7.7319,\n",
      "         1.1145, 6.3456, 5.6525, 1.9543, 2.2939, 5.7860, 7.0388, 5.3340, 7.7319,\n",
      "         1.3969, 6.6333, 0.5534, 0.0000]])\n",
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "#vid_caps='Outputs the deferred summary information saved via @logger.summary.add_action and @logger.summary.add_paragraph'\n",
    "\n",
    "vid_caps='A person is frying a pan of eggs that are colored pink and green.'\n",
    "device=\"cuda:0\"\n",
    "text_features,mask,idf_weights=encode_text(vid_caps,model,tokenizer,idf_dict,device)\n",
    "print(\"文本处理之后的一些信息：\")\n",
    "print(type(text_features))\n",
    "print(text_features)\n",
    "print(text_features.shape)\n",
    "\n",
    "print(\"关于mask的一些信息\")\n",
    "print(type(mask))\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "\n",
    "print(\"关于idf权重的一些信息：\")\n",
    "print(type(idf_weights))\n",
    "print(idf_weights)\n",
    "print(idf_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文本处理的部分处理完了，接下来应该把原来对video进行处理的部分改成对code进行处理，但本质上对code处理的过程应该跟text很类似，只是得到code中每个token的向量后，如何将其组合得到不同粒度的向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "196005e07f823faab020780d1971fd9ef55151a4e2624f761e89102bc97b9bea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
