{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-26 21:20:48--  https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 0.0.0.0, ::\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|0.0.0.0|:443... failed: Connection refused.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|::|:443... failed: Connection refused.\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniXcoder(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(51416, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51416, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from unixcoder import UniXcoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  re\n",
    "from torch import per_channel_affine\n",
    "from tqdm import tqdm\n",
    "import time, random\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import log\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取数据文件（先找个小一点的jsonl文件放到服务器上，选择CSN里ruby语言的test文件比较小）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained unixcoder tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n",
    "model = AutoModel.from_pretrained('microsoft/unixcoder-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279\n"
     ]
    }
   ],
   "source": [
    "#input:code snippet\n",
    "import jsonlines\n",
    "\n",
    "test_list=[]\n",
    "test_file=\"/home/huangyx/emscore/emscore/dataset/ruby_test_0.jsonl\"\n",
    "with open(test_file,mode='r') as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        test_list.append(item)\n",
    "print(len(test_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def build_sale_data(sale_item)\n",
      "        \"\".b.tap() do |data|\n",
      "          price_units = (sale_item.price * 100).to_i # !FIXME\n",
      "          price_bytes = \"\".b\n",
      "          4.times{ |shift| price_bytes.insert 0, ((price_units >> shift*8) & 0xff).chr }\n",
      "          data << price_bytes\n",
      "          qty_units = ((sale_item.qty || 1) * 1000).to_i # !FIXME\n",
      "          qty_bytes = \"\".b\n",
      "          4.times{ |shift| qty_bytes.insert 0, ((qty_units >> shift*8) & 0xff).chr }\n",
      "          data << qty_bytes\n",
      "          data << \"\\x00\".b #number len FIXME\n",
      "          data << \"\\xAA\\xAA\\xAA\\xAA\\xAA\\xAA\".b #number FIXME\n",
      "          text = sale_item.text1.truncate(20)\n",
      "          data << text.length.chr\n",
      "          data << text.ljust(20, \" \").b\n",
      "          data << (sale_item.tax_group || 2).chr\n",
      "        end\n",
      "      end\n",
      "----------------------------------------\n",
      "def build_sale_data(price, text1 = \"\", text2 = nil, tax_group = 2, qty = 1, percent = nil, neto = nil, number = nil)\n"
     ]
    }
   ],
   "source": [
    "#gain sample\n",
    "import json\n",
    "\n",
    "sample_list=random.sample(test_list,10)\n",
    "print(sample_list[0]['code'])\n",
    "print(\"----------------------------------------\")\n",
    "print(sample_list[0]['docstring'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到code和candidate的embedding向量,output_code/ouput_candidate就包含了每一个token/word向量\n",
    "#code_embeddings  candidate_embeddings就是分别对应整个向量表示，这里取的是mean操作获得\n",
    "\n",
    "\n",
    "\n",
    "# Encode input code and obtain word embeddings\n",
    "input_code = sample_list[0]['code']\n",
    "encoded_code = tokenizer.encode_plus(input_code, return_tensors='pt')\n",
    "output_code = model(**encoded_code)[0]\n",
    "code_embeddings = output_code.mean(dim=1)\n",
    "\n",
    "# Encode input candidate and obtain word embeddings\n",
    "input_candidate = sample_list[0]['docstring']\n",
    "encoded_candidate = tokenizer.encode_plus(input_candidate, return_tensors='pt')\n",
    "output_candidate = model(**encoded_candidate)[0]\n",
    "candidate_embeddings = output_candidate.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_summary(status)\n",
      "      status_string = status.to_s.humanize.upcase\n",
      "      if status == :success\n",
      "        heading(\"Result: \", status_string, :green)\n",
      "        level = :info\n",
      "      elsif status == :timed_out\n",
      "        heading(\"Result: \", status_string, :yellow)\n",
      "        level = :fatal\n",
      "      else\n",
      "        heading(\"Result: \", status_string, :red)\n",
      "        level = :fatal\n",
      "      end\n",
      "\n",
      "      if (actions_sentence = summary.actions_sentence.presence)\n",
      "        public_send(level, actions_sentence)\n",
      "        blank_line(level)\n",
      "      end\n",
      "\n",
      "      summary.paragraphs.each do |para|\n",
      "        msg_lines = para.split(\"\\n\")\n",
      "        msg_lines.each { |line| public_send(level, line) }\n",
      "        blank_line(level) unless para == summary.paragraphs.last\n",
      "      end\n",
      "    end\n",
      "-------------------\n",
      "Outputs the deferred summary information saved via @logger.summary.add_action and @logger.summary.add_paragraph\n"
     ]
    }
   ],
   "source": [
    "print(test_list[1]['code'] )\n",
    "print(\"-------------------\")\n",
    "print(test_list[1]['docstring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_code.shape)\n",
    "print(output_candidate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里的a参数指的是每一个candidate文本句子\n",
    "\n",
    "def process(a, tokenizer=None):\n",
    "    if tokenizer is not None:\n",
    "        a1 = tokenizer.encode(a, add_special_tokens=True, return_tensors=\"pt\").tolist()[0]\n",
    "    return set(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算词频，一共N个句子，统计单词在这些句子里出现几次，出现在一个句子里算一次，输入一个corpus里所有句子，输出所有单词对应的词频\n",
    "\n",
    "def get_idf_dict(arr, tokenizer, nthreads=4):\n",
    "    \"\"\"\n",
    "    Returns mapping from word piece index to its inverse document frequency.\n",
    "    Args:\n",
    "        - :param: `arr` (list of str) : sentences to process.\n",
    "        - :param: `tokenizer` : a BERT tokenizer corresponds to `model`.\n",
    "        - :param: `nthreads` (int) : number of CPU threads to use\n",
    "    \"\"\"\n",
    "    idf_count = Counter()\n",
    "    num_docs = len(arr)\n",
    "\n",
    "    process_partial = partial(process, tokenizer=tokenizer)\n",
    "\n",
    "    with Pool(nthreads) as p:\n",
    "        idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n",
    "\n",
    "    idf_dict = defaultdict(lambda: log((num_docs + 1) / (1)))\n",
    "    idf_dict.update({idx: log((num_docs + 1) / (c + 1)) for (idx, c) in idf_count.items()})\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_list=[]\n",
    "for item in test_list:\n",
    "    candidate_list.append(item['docstring'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算得到每个token对应的idf结果\n",
    "\n",
    "idf_dict=get_idf_dict(candidate_list,tokenizer)\n",
    "print(idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_list=[]\n",
    "for item in test_list:\n",
    "    code_list.append(item['code'])\n",
    "code_idf_dict=get_idf_dict(code_list,tokenizer)\n",
    "print(code_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\n",
    "model = AutoModel.from_pretrained('microsoft/unixcoder-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(vid_caps, model, tokenizer, idf_dict, device):\n",
    "    with torch.no_grad():\n",
    "        text_input = tokenizer.encode_plus(vid_caps, return_tensors='pt',return_attention_mask=True)\n",
    "\n",
    "        text_features = model(**text_input)[0]\n",
    "    #mask= text_input['attention_mask']\n",
    "\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # For special tokens, use [SOS] and [EOS]\n",
    "    txt_len = text_input['input_ids'].argmax(dim=-1)\n",
    "    mask = torch.zeros_like(text_input['input_ids'])\n",
    "    for i in range(len(mask)):\n",
    "        mask[i][0:txt_len[i]+1] = 1\n",
    "\n",
    "    idf_weights = torch.tensor([[idf_dict[int(i)] for i in a] for a in text_input['input_ids'].cpu()])\n",
    "\n",
    "    return text_features, mask, idf_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本处理之后的一些信息：\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[-0.0204, -0.0282, -0.0100,  ..., -0.0776, -0.0652, -0.0834],\n",
      "         [-0.0204, -0.0282, -0.0100,  ..., -0.0776, -0.0652, -0.0834],\n",
      "         [-0.0204, -0.0282, -0.0100,  ..., -0.0776, -0.0652, -0.0834],\n",
      "         ...,\n",
      "         [-0.0229,  0.0105,  0.0403,  ..., -0.0697, -0.0601, -0.0484],\n",
      "         [ 0.0118,  0.0317,  0.0442,  ..., -0.0386, -0.0558, -0.0312],\n",
      "         [-0.0548, -0.0081,  0.0229,  ..., -0.0307, -0.0644, -0.0186]]])\n",
      "torch.Size([1, 26, 768])\n",
      "关于mask的一些信息\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0]])\n",
      "torch.Size([1, 26])\n",
      "关于idf权重的一些信息：\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.0000, 5.9402, 0.5549, 7.0388, 6.3456, 3.7246, 5.3340, 4.6409, 1.1307,\n",
      "         5.4293, 0.5534, 7.0388, 0.5534, 4.3646, 1.1569, 5.6525, 1.3969, 1.1307,\n",
      "         5.4293, 0.5534, 7.0388, 0.5534, 4.3646, 1.1569, 6.6333, 0.0000]])\n",
      "torch.Size([1, 26])\n"
     ]
    }
   ],
   "source": [
    "vid_caps='Outputs the deferred summary information saved via @logger.summary.add_action and @logger.summary.add_paragraph'\n",
    "\n",
    "#vid_caps='A person is frying a pan of eggs that are colored pink and green.'\n",
    "device=\"cuda:0\"\n",
    "text_features,mask,idf_weights=encode_text(vid_caps,model,tokenizer,idf_dict,device)\n",
    "print(\"文本处理之后的一些信息：\")\n",
    "print(type(text_features))\n",
    "print(text_features)\n",
    "print(text_features.shape)\n",
    "\n",
    "print(\"关于mask的一些信息\")\n",
    "print(type(mask))\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "\n",
    "print(\"关于idf权重的一些信息：\")\n",
    "print(type(idf_weights))\n",
    "print(idf_weights)\n",
    "print(idf_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文本处理的部分处理完了，接下来应该把原来对video进行处理的部分改成对code进行处理，但本质上对code处理的过程应该跟text很类似，只是得到code中每个token的向量后，如何将其组合得到不同粒度的向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一步应该类似text的操作，先获得code中每个token的embedding（token_level）以及整个code snippet的embedding表示（function_level）\n",
    "def encode_code(code, model, tokenizer, code_idf_dict, device):\n",
    "    with torch.no_grad():\n",
    "        code_input = tokenizer.encode_plus(code, return_tensors='pt',return_attention_mask=True)\n",
    "\n",
    "        code_features = model(**code_input)[0]\n",
    "\n",
    "\n",
    "    code_features /= code_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # For special tokens, use [SOS] and [EOS]\n",
    "    code_len = code_input['input_ids'].argmax(dim=-1)\n",
    "    code_mask = torch.zeros_like(code_input['input_ids'])\n",
    "    for i in range(len(code_mask)):\n",
    "        code_mask[i][0:code_len[i]+1] = 1\n",
    "\n",
    "    code_idf_weights = torch.tensor([[code_idf_dict[int(i)] for i in a] for a in code_input['input_ids'].cpu()])\n",
    "\n",
    "    return code_features, code_mask, code_idf_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码处理之后的一些信息：\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[-0.0846, -0.0142,  0.0101,  ..., -0.0234,  0.0021, -0.0564],\n",
      "         [-0.0846, -0.0142,  0.0101,  ..., -0.0234,  0.0021, -0.0564],\n",
      "         [-0.0846, -0.0142,  0.0101,  ..., -0.0234,  0.0021, -0.0564],\n",
      "         ...,\n",
      "         [-0.0325,  0.0659,  0.0066,  ..., -0.0075,  0.0174, -0.0476],\n",
      "         [-0.0238,  0.0480, -0.0140,  ...,  0.0212,  0.0016, -0.0897],\n",
      "         [-0.0091, -0.0623,  0.0475,  ..., -0.0169,  0.0185,  0.0290]]])\n",
      "torch.Size([1, 215, 768])\n",
      "关于mask的一些信息\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "torch.Size([1, 215])\n",
      "关于idf权重的一些信息：\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[0.0000, 0.0000, 5.0929, 0.0421, 5.4293, 0.1636, 3.6889, 0.1411, 0.0000,\n",
      "         0.0426, 4.3646, 0.0421, 3.5575, 0.2303, 4.3646, 0.0799, 1.2367, 0.0421,\n",
      "         1.5877, 0.0799, 5.7860, 3.5124, 0.0799, 5.1670, 0.0000, 0.0426, 0.6107,\n",
      "         4.3646, 1.8375, 1.3706, 5.3340, 0.0000, 0.4254, 6.6333, 2.1751, 6.3456,\n",
      "         1.2113, 6.1225, 4.3646, 0.0421, 3.5575, 0.2403, 1.3706, 5.2470, 0.1411,\n",
      "         0.0000, 0.4254, 4.7875, 0.2303, 1.3706, 3.1368, 0.0000, 0.0426, 2.9870,\n",
      "         4.3646, 1.8375, 1.3706, 6.1225, 0.0421, 4.1210, 0.0000, 0.4254, 6.6333,\n",
      "         2.1751, 6.3456, 1.2113, 6.1225, 4.3646, 0.0421, 3.5575, 0.2403, 1.3706,\n",
      "         5.9402, 0.1411, 0.0000, 0.4254, 4.7875, 0.2303, 1.3706, 5.1670, 0.0000,\n",
      "         0.0426, 1.3333, 0.0000, 0.4254, 6.6333, 2.1751, 6.3456, 1.2113, 6.1225,\n",
      "         4.3646, 0.0421, 3.5575, 0.2403, 1.3706, 5.0239, 0.1411, 0.0000, 0.4254,\n",
      "         4.7875, 0.2303, 1.3706, 5.1670, 0.0000, 0.0426, 0.0217, 0.0000, 0.0000,\n",
      "         0.0426, 0.6107, 1.8740, 4.6409, 0.0421, 5.9402, 0.2303, 5.4293, 0.0799,\n",
      "         4.6409, 0.0421, 5.9402, 0.0799, 4.3997, 0.1411, 0.0000, 0.4254, 5.2470,\n",
      "         0.0421, 3.1070, 0.1636, 4.3646, 0.2403, 5.5347, 0.0421, 5.9402, 0.1411,\n",
      "         0.0000, 0.4254, 5.7860, 0.0421, 3.7616, 0.1636, 4.3646, 0.1411, 0.0000,\n",
      "         0.0426, 0.0217, 0.0000, 0.0000, 0.0426, 5.4293, 0.0799, 5.9402, 1.5877,\n",
      "         0.0799, 1.6227, 1.0881, 1.0117, 6.6333, 0.9543, 0.0000, 0.4254, 4.6874,\n",
      "         0.0421, 4.9593, 0.2303, 6.3456, 0.0799, 3.2546, 4.0684, 2.7761, 2.0690,\n",
      "         0.0000, 0.4254, 4.6874, 0.0421, 4.9593, 0.0799, 1.6227, 1.3103, 1.0117,\n",
      "         3.7616, 0.9543, 5.2470, 0.0421, 3.1070, 0.1636, 4.3646, 0.2403, 3.9253,\n",
      "         0.1411, 1.4478, 0.0000, 0.4254, 5.7860, 0.0421, 3.7616, 0.1636, 4.3646,\n",
      "         0.1411, 1.5728, 6.3456, 1.8375, 5.4293, 0.0799, 5.9402, 1.5877, 0.0799,\n",
      "         3.2321, 0.0000, 0.0426, 0.0217, 0.0000, 0.0444, 0.0217, 0.0000]])\n",
      "torch.Size([1, 215])\n"
     ]
    }
   ],
   "source": [
    "code=test_list[1]['code']\n",
    "\n",
    "device=\"cuda:0\"\n",
    "code_features,code_mask,code_idf_weights=encode_code(code,model,tokenizer,code_idf_dict,device)\n",
    "print(\"代码处理之后的一些信息：\")\n",
    "print(type(code_features))\n",
    "print(code_features)\n",
    "print(code_features.shape)\n",
    "\n",
    "print(\"关于mask的一些信息\")\n",
    "print(type(code_mask))\n",
    "print(code_mask)\n",
    "print(code_mask.shape)\n",
    "\n",
    "print(\"关于idf权重的一些信息：\")\n",
    "print(type(code_idf_weights))\n",
    "print(code_idf_weights)\n",
    "print(code_idf_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing text embedding.\n",
      "['Outputs the deferred summary information saved via @logger.summary.add_action and @logger.summary.add_paragraph']\n",
      "<class 'list'>\n",
      "sequence_len的值是：\n",
      "25\n",
      "此处获得local embedding向量：\n",
      "tensor([[-0.0204, -0.0282, -0.0100,  ..., -0.0776, -0.0652, -0.0834],\n",
      "        [-0.0204, -0.0282, -0.0100,  ..., -0.0776, -0.0652, -0.0834],\n",
      "        [-0.0204, -0.0282, -0.0100,  ..., -0.0776, -0.0652, -0.0834],\n",
      "        ...,\n",
      "        [-0.0251,  0.0308,  0.0083,  ..., -0.0389, -0.0996, -0.0329],\n",
      "        [-0.0229,  0.0105,  0.0403,  ..., -0.0697, -0.0601, -0.0484],\n",
      "        [ 0.0118,  0.0317,  0.0442,  ..., -0.0386, -0.0558, -0.0312]])\n",
      "torch.Size([25, 768])\n",
      " 此处获得global embedding向量：\n",
      "tensor([ 1.1834e-02,  3.1693e-02,  4.4249e-02, -2.4906e-02,  4.6708e-02,\n",
      "        -3.3223e-02,  3.4815e-02,  4.3800e-03,  6.3374e-02,  2.3459e-02,\n",
      "        -2.7429e-03, -6.7016e-02, -4.9030e-02,  5.8587e-02,  2.5201e-02,\n",
      "         3.4271e-02, -6.8927e-02,  2.3633e-02,  3.0989e-02, -2.1847e-02,\n",
      "         3.3886e-02,  7.5957e-02, -1.7830e-02,  1.4574e-02, -5.8476e-02,\n",
      "         6.5744e-03, -1.3352e-02, -3.6657e-02, -4.7555e-03, -4.1635e-03,\n",
      "        -9.0221e-02, -1.0026e-02, -6.0949e-02, -2.8267e-03, -5.6868e-02,\n",
      "        -3.2141e-03, -6.1587e-03,  3.4017e-02,  8.8784e-04, -3.0027e-02,\n",
      "         1.9521e-02, -1.9767e-03, -1.1137e-02, -7.9213e-03, -5.3709e-02,\n",
      "        -8.3342e-03,  7.8739e-04,  4.5151e-03,  2.3455e-02,  5.3142e-03,\n",
      "         4.0851e-02,  4.9495e-02, -2.6565e-02,  3.6568e-02,  1.5361e-02,\n",
      "         9.2896e-04, -1.9476e-02,  6.1405e-02,  3.8406e-02,  5.3373e-02,\n",
      "         2.7742e-02,  1.3033e-02, -3.9293e-03,  1.2399e-02,  3.0473e-02,\n",
      "         2.6243e-02, -1.9866e-02,  1.1852e-02, -3.5881e-02,  1.4407e-02,\n",
      "         5.5223e-03, -5.7123e-03, -9.2644e-03, -3.7315e-02, -4.5937e-02,\n",
      "        -4.8191e-02,  5.9683e-02, -1.2977e-03,  3.8708e-02, -4.9180e-02,\n",
      "         6.7438e-02,  2.7118e-02,  3.3177e-03, -7.7753e-03, -2.6722e-02,\n",
      "         1.0647e-02,  1.3604e-02,  1.9568e-02, -3.9500e-02, -2.4921e-02,\n",
      "         6.2644e-03,  2.0749e-02, -4.9997e-03, -5.7995e-03, -9.3682e-03,\n",
      "         1.5073e-02,  3.9022e-02,  4.7850e-02, -2.2272e-02,  7.3930e-03,\n",
      "         9.3550e-02, -3.3402e-03, -1.4941e-04, -4.6583e-02,  1.9833e-02,\n",
      "        -1.1132e-02,  4.1577e-03,  2.2429e-02,  3.2198e-02,  1.2241e-02,\n",
      "        -1.5067e-02,  1.7823e-02,  5.0794e-02,  1.4509e-02, -8.7717e-02,\n",
      "         2.4034e-02, -5.7063e-03, -5.1286e-02, -5.7549e-02, -3.3890e-02,\n",
      "         4.9539e-03, -6.4091e-03,  3.3809e-02, -1.3068e-03, -3.3068e-02,\n",
      "        -9.4182e-03,  5.1980e-02, -5.7618e-02, -4.4943e-03, -1.4816e-02,\n",
      "         3.2073e-02, -3.4215e-02,  3.7483e-02,  2.5589e-02, -5.2866e-02,\n",
      "         2.1882e-02,  3.2466e-02,  1.3418e-03,  9.0176e-04, -1.1630e-02,\n",
      "        -3.1162e-02,  1.7925e-02,  2.4813e-03,  3.2536e-02, -4.6873e-02,\n",
      "        -3.6665e-02, -1.0345e-02, -1.4084e-02,  3.0338e-02, -3.8899e-02,\n",
      "         1.3374e-02, -1.1870e-02, -1.5722e-02,  2.8228e-02,  4.4385e-02,\n",
      "         2.5511e-02, -8.7635e-03, -1.2640e-02, -6.3817e-02, -1.7495e-02,\n",
      "         2.3807e-02, -4.9682e-02,  6.6164e-03, -1.0972e-02,  9.2197e-03,\n",
      "        -1.4650e-02, -2.6455e-02, -3.8564e-02,  1.2663e-02, -4.8008e-02,\n",
      "         3.8359e-02,  4.7677e-02,  2.5380e-02, -9.1134e-03,  6.5577e-02,\n",
      "        -1.1784e-02,  2.5313e-02, -2.9988e-02,  3.6209e-04,  2.2639e-02,\n",
      "        -3.9891e-02, -2.1180e-02,  8.0253e-02,  8.0926e-03, -2.7887e-02,\n",
      "         1.7338e-02, -1.9453e-02,  2.6969e-02, -1.2031e-02, -4.4240e-02,\n",
      "         3.3866e-02, -7.7383e-03,  3.3637e-03,  6.0353e-03, -5.1228e-02,\n",
      "         1.2845e-02,  2.7276e-02,  2.5412e-02,  2.4171e-02, -1.8926e-03,\n",
      "        -2.6734e-02, -2.0019e-02,  1.2636e-02, -8.2273e-02,  8.0206e-02,\n",
      "         1.2878e-03,  9.8202e-03, -6.8624e-02, -1.6556e-02,  1.3230e-02,\n",
      "        -3.0667e-02, -2.0432e-02, -1.2413e-02,  3.8564e-02, -1.8890e-02,\n",
      "         1.7724e-02,  7.0963e-02, -2.0286e-02,  4.1135e-03, -1.0131e-02,\n",
      "        -2.3842e-02,  1.9431e-02,  3.5622e-02,  4.1064e-02, -6.3054e-03,\n",
      "        -5.6061e-02, -2.2582e-02, -8.1784e-03, -3.5452e-02, -1.0268e-02,\n",
      "         3.3596e-02,  3.1475e-03,  1.0592e-02, -2.2851e-02,  5.3600e-03,\n",
      "         2.0788e-02,  1.6970e-02,  4.5890e-03, -3.9350e-02, -1.9855e-02,\n",
      "         1.5635e-02, -1.4063e-02, -3.7784e-02, -3.5477e-03,  4.8115e-02,\n",
      "        -1.5680e-02,  3.2857e-02, -5.7199e-03,  1.2822e-01,  3.8145e-02,\n",
      "        -2.9283e-02, -3.7150e-03,  3.8231e-02,  2.0689e-02,  2.0920e-02,\n",
      "        -5.0175e-02,  3.3763e-02,  2.0629e-02, -4.3122e-02, -5.7349e-02,\n",
      "        -1.2301e-02, -2.1541e-02, -2.6364e-02, -4.8596e-02,  3.6594e-02,\n",
      "        -3.2261e-02,  3.1400e-02, -1.8644e-02, -4.2646e-02,  2.8463e-02,\n",
      "         3.3855e-02,  1.5737e-02, -5.5095e-03, -1.4608e-02, -2.3848e-02,\n",
      "         1.5050e-02,  4.9501e-02, -1.8898e-02,  5.2804e-02,  1.7148e-02,\n",
      "         2.3205e-02,  3.9951e-02, -2.6060e-02,  1.0111e-02,  2.7758e-02,\n",
      "         1.3544e-02,  7.9204e-02,  4.9548e-02,  1.0828e-02,  2.2556e-02,\n",
      "        -4.9479e-02,  6.7407e-03,  5.6449e-02, -5.3147e-03,  1.1454e-02,\n",
      "        -1.8963e-02, -1.1169e-04, -6.4946e-02, -4.2800e-03, -1.7887e-02,\n",
      "         4.6469e-02,  1.7563e-02, -1.4198e-02, -6.6231e-02,  4.1081e-02,\n",
      "         3.0186e-02, -2.2986e-02,  2.6037e-02, -7.0780e-02, -3.4258e-02,\n",
      "         8.8053e-03,  3.4958e-02,  3.3224e-02,  3.5420e-02, -2.9474e-02,\n",
      "        -4.7355e-02, -4.0274e-02, -2.5027e-02,  7.9297e-03, -6.9723e-02,\n",
      "        -3.7234e-03,  2.7099e-02, -3.2053e-02,  1.2568e-02,  1.5224e-02,\n",
      "        -2.0314e-02, -2.9278e-02,  3.6514e-02, -3.7207e-02,  1.3404e-02,\n",
      "         9.1109e-02,  8.2315e-03,  4.8896e-02,  3.7418e-03, -1.7938e-03,\n",
      "        -3.3738e-03,  2.4746e-02,  7.7337e-03, -4.6483e-02,  8.7104e-04,\n",
      "        -3.1134e-03,  6.0232e-02, -2.8973e-02,  1.2918e-02,  1.9181e-02,\n",
      "        -1.8978e-02, -1.9750e-02,  2.3502e-02,  4.3477e-02, -6.2031e-03,\n",
      "        -3.0389e-02,  8.9959e-03, -8.0275e-03, -3.3943e-03, -2.4376e-02,\n",
      "         1.2773e-02, -1.2283e-01,  2.3955e-02,  4.5063e-02, -1.0832e-02,\n",
      "        -4.2176e-02,  1.1608e-02,  5.4170e-03,  3.2092e-02, -5.0259e-03,\n",
      "        -2.6977e-02,  1.1432e-02, -4.5162e-03,  6.7048e-02, -8.4508e-03,\n",
      "        -4.3577e-02,  2.9313e-02,  2.9502e-02,  1.9779e-02, -6.5107e-02,\n",
      "         7.9331e-03, -3.0492e-02,  6.4113e-03,  1.8370e-02, -5.3198e-02,\n",
      "         4.2597e-02, -5.3335e-03,  3.9174e-02, -6.0252e-03, -2.4121e-02,\n",
      "         4.9351e-02,  4.6867e-02,  1.9215e-03, -1.4976e-02,  5.8826e-03,\n",
      "        -2.4667e-02, -3.5237e-02,  4.9339e-02, -2.1104e-03, -1.5112e-02,\n",
      "         1.9239e-02, -1.4620e-04,  5.8420e-02,  3.7352e-03,  4.0686e-02,\n",
      "        -3.1633e-02,  2.6136e-03, -2.4359e-02, -2.3090e-02, -5.9595e-03,\n",
      "         4.1659e-02, -2.5168e-02,  1.4915e-02,  1.0733e-02,  1.0221e-02,\n",
      "        -4.8176e-02,  9.7316e-03, -1.2139e-02,  3.0209e-02, -5.3669e-02,\n",
      "         5.2428e-02,  4.3959e-02,  9.6312e-03, -4.6204e-02,  6.1895e-03,\n",
      "         2.7760e-02,  1.8122e-02, -3.4756e-02,  4.4121e-02, -3.7576e-02,\n",
      "        -1.2121e-02, -1.0819e-03, -1.8150e-02, -3.2853e-02, -1.1257e-02,\n",
      "         3.2169e-02, -4.7187e-02,  3.4421e-03, -3.1490e-02,  5.8037e-02,\n",
      "         2.0178e-03,  1.5482e-02,  3.1286e-02,  1.3096e-02,  2.6213e-02,\n",
      "        -7.6054e-02,  4.4675e-02, -9.3623e-03,  8.2746e-02, -4.0366e-02,\n",
      "         2.2678e-02, -3.1596e-02,  5.4858e-02,  3.8618e-02, -8.9687e-03,\n",
      "         6.1864e-03, -3.0643e-02, -4.9132e-03,  1.9136e-02,  4.5038e-02,\n",
      "        -5.2483e-03, -1.4322e-02, -1.5757e-02, -6.6086e-03, -5.5701e-02,\n",
      "        -1.2387e-02,  6.4950e-02, -5.0045e-02, -1.1877e-02,  1.6123e-02,\n",
      "         1.6115e-02, -2.3577e-02,  2.7279e-04, -3.0717e-03,  2.5216e-02,\n",
      "         4.7412e-03,  9.3523e-03, -1.5385e-02,  1.9257e-02,  5.5226e-03,\n",
      "         7.3833e-03,  1.0813e-02, -4.6376e-02,  2.2869e-02,  2.2324e-02,\n",
      "         2.5279e-02, -4.4734e-02, -7.5305e-03,  3.9170e-02,  1.2706e-03,\n",
      "        -3.3688e-02, -4.9011e-02,  1.3190e-03,  2.0030e-02, -2.9129e-02,\n",
      "        -1.9820e-02, -1.9683e-02,  2.4178e-02, -2.9854e-02, -4.2755e-02,\n",
      "         1.6967e-02, -5.3669e-03, -6.3121e-02, -2.0643e-02,  5.6068e-02,\n",
      "        -4.5094e-03, -3.3657e-02, -1.8721e-02, -6.3597e-02, -1.9217e-02,\n",
      "         8.1476e-03,  4.8103e-02, -2.1440e-02, -2.0528e-02,  4.2996e-02,\n",
      "        -1.4067e-02,  3.9130e-02,  1.3059e-02, -1.3008e-02,  3.1474e-02,\n",
      "         3.1691e-04,  3.8961e-02,  2.4319e-02,  4.9907e-03, -3.6503e-02,\n",
      "        -1.5798e-02,  3.9310e-02,  2.1943e-02,  4.0021e-02,  5.0457e-02,\n",
      "        -2.7287e-02, -1.3499e-02, -8.2969e-02, -1.8457e-02,  7.3945e-03,\n",
      "         3.7603e-01,  1.0787e-02,  1.6333e-02,  4.2061e-03, -2.2929e-02,\n",
      "        -2.7596e-02, -1.3497e-02,  3.1190e-02,  6.5273e-03,  1.3529e-03,\n",
      "         3.8724e-02,  3.3816e-02, -3.5192e-02, -6.6546e-02,  6.5169e-02,\n",
      "         3.4123e-02, -1.3934e-03, -1.7463e-02,  3.6587e-03,  9.6558e-02,\n",
      "         1.9225e-02,  2.2399e-02,  1.5022e-02, -2.3839e-02, -4.7627e-04,\n",
      "         1.8404e-02,  3.5699e-02,  8.0790e-04,  2.6880e-02, -6.4495e-02,\n",
      "        -9.8180e-04, -2.2131e-02,  6.7575e-02,  6.9037e-02, -9.7957e-02,\n",
      "        -1.4207e-02, -4.0935e-02, -6.6777e-02, -2.7627e-02,  5.2226e-02,\n",
      "         2.5012e-03,  1.8910e-02, -4.7923e-02,  7.1304e-02,  3.2343e-02,\n",
      "         3.5535e-03,  1.5828e-02, -5.6934e-02, -2.0467e-02, -2.3121e-02,\n",
      "         1.0606e-02, -9.7579e-03,  2.5037e-02, -1.1158e-02, -1.3457e-02,\n",
      "         1.7912e-03,  2.0431e-02, -6.2584e-03, -2.1931e-02,  3.3311e-02,\n",
      "         7.2431e-03,  4.2135e-02,  2.1644e-02, -4.4196e-02, -9.4424e-03,\n",
      "        -2.2823e-02,  4.9650e-02, -6.1876e-03,  2.1517e-02,  1.0516e-01,\n",
      "         4.7617e-02,  4.4319e-02, -2.8168e-02,  7.1153e-02,  4.6232e-02,\n",
      "        -2.2927e-02, -4.5010e-02, -1.2982e-02,  3.8574e-02,  2.8742e-02,\n",
      "         2.9428e-02,  1.0490e-01, -7.7403e-02, -7.6610e-02, -1.8002e-02,\n",
      "         1.5660e-02, -5.6772e-02,  6.0013e-03,  5.2537e-02,  6.1960e-03,\n",
      "         2.9504e-02,  4.7651e-02,  3.9729e-02, -5.7303e-02, -1.1819e-02,\n",
      "         2.0630e-02,  7.2802e-03, -2.6037e-02,  1.1758e-02,  1.0874e-02,\n",
      "        -3.0085e-02, -3.2695e-02, -1.5985e-02,  2.3256e-02, -1.0507e-02,\n",
      "         4.6440e-02, -2.0588e-02,  4.7628e-02,  1.3846e-02, -5.2995e-03,\n",
      "         5.0699e-02,  1.2785e-02, -1.1718e-02, -2.7544e-02, -6.8423e-02,\n",
      "        -1.6823e-02,  4.9009e-02, -2.0923e-02, -2.6191e-02, -3.1861e-03,\n",
      "        -2.7084e-02,  1.0266e-02, -1.9812e-02,  2.2557e-02, -1.1439e-02,\n",
      "        -3.5930e-02,  2.4231e-02, -7.6681e-03, -5.6808e-03,  1.3155e-02,\n",
      "        -8.1475e-03,  1.0809e-02,  1.9233e-02,  1.2929e-02,  1.6081e-02,\n",
      "        -1.6086e-02,  4.4599e-03,  7.8313e-02, -4.0682e-02, -4.6720e-02,\n",
      "        -2.9241e-02, -2.2126e-02,  1.0904e-02,  7.6022e-03, -3.3314e-02,\n",
      "         2.7040e-02,  3.6726e-02, -2.2473e-02, -3.4130e-02,  1.6965e-02,\n",
      "        -9.9152e-03, -1.3460e-02, -2.4637e-02,  1.4163e-02,  1.0506e-03,\n",
      "         6.3897e-02, -1.9743e-03,  2.8530e-02, -2.1878e-02, -8.2348e-03,\n",
      "         3.6066e-02, -1.7905e-02, -1.7711e-02, -1.3227e-02,  1.1096e-02,\n",
      "        -6.2334e-04,  2.9325e-04,  4.2764e-02,  2.0514e-02, -1.0628e-02,\n",
      "         4.7750e-03, -9.3591e-03, -5.7469e-02, -3.5881e-03,  4.2711e-02,\n",
      "        -6.0215e-02, -2.5461e-02,  1.5149e-03, -2.2464e-02, -8.1097e-03,\n",
      "         9.0801e-03, -1.9595e-02, -2.7896e-02,  2.5883e-02,  1.7982e-02,\n",
      "        -4.0588e-02,  2.1078e-02, -4.0270e-02, -7.8819e-03, -2.5487e-03,\n",
      "         1.1820e-02,  4.7713e-02,  6.0921e-02,  3.2227e-02, -5.1772e-02,\n",
      "        -4.3613e-02,  3.5402e-02, -7.4663e-03,  8.4209e-03, -3.8303e-02,\n",
      "        -3.2131e-02,  3.4330e-03,  4.6322e-02,  2.3745e-02, -2.4505e-02,\n",
      "        -1.5276e-02,  6.8379e-02,  3.3528e-02, -7.8916e-03,  1.6259e-02,\n",
      "         2.3874e-02,  6.4848e-04, -1.2712e-02,  4.0293e-03, -3.0623e-02,\n",
      "        -1.3013e-02, -1.1073e-02, -5.1687e-02, -8.0815e-03,  1.4881e-02,\n",
      "         7.4237e-03,  1.9631e-02,  1.2182e-02,  8.6837e-03, -2.4409e-02,\n",
      "        -1.5603e-02, -2.7830e-02,  1.3761e-02, -4.0322e-02,  2.4089e-02,\n",
      "         3.4387e-02, -3.8668e-02,  2.0218e-02, -1.5978e-02,  4.4810e-02,\n",
      "        -3.8566e-02, -5.5774e-02, -3.1229e-02])\n",
      "torch.Size([768])\n",
      "获得的idf值：\n",
      "tensor([0.0000, 5.9402, 0.5549, 7.0388, 6.3456, 3.7246, 5.3340, 4.6409, 1.1307,\n",
      "        5.4293, 0.5534, 7.0388, 0.5534, 4.3646, 1.1569, 5.6525, 1.3969, 1.1307,\n",
      "        5.4293, 0.5534, 7.0388, 0.5534, 4.3646, 1.1569, 6.6333])\n",
      "torch.Size([25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hyps=[]\n",
    "hyps.append(vid_caps)\n",
    "\n",
    "verbose=True\n",
    "batch_size=64\n",
    "return_matched_idx=False\n",
    "\n",
    "\n",
    "refs_preds_local = []\n",
    "refs_pred_matched_idxs = []\n",
    "refs_preds_global = []\n",
    "\n",
    "vid_preds_local = []\n",
    "vid_pred_matched_idxs = []\n",
    "vid_preds_global = []\n",
    "\n",
    "\"\"\"process text\"\"\"\n",
    "def dedup_and_sort(l):\n",
    "    return sorted(list(set(l)), key=lambda x: len(x.split(\" \")), reverse=True)\n",
    "\n",
    "sentences = dedup_and_sort(hyps)\n",
    "#sentences = dedup_and_sort(hyps)\n",
    "\n",
    "\n",
    "embs = []\n",
    "iter_range = range(0, len(sentences), batch_size)\n",
    "if verbose:\n",
    "    print(\"computing text embedding.\")\n",
    "    iter_range = tqdm(iter_range)\n",
    "text_local_stats_dict = dict()\n",
    "text_global_stats_dict = dict()\n",
    "for batch_start in iter_range:\n",
    "    sen_batch = sentences[batch_start: batch_start + batch_size]\n",
    "    print(sen_batch)\n",
    "    print(type(sen_batch))\n",
    "    #实际处理文本内容在这里调用encode_text\n",
    "    #此处将encode_text函数替换成处理代码任务中candidate的函数\n",
    "    embs, masks, text_idfs = encode_text(sen_batch[0], model, tokenizer, idf_dict, device=device)\n",
    "    embs = embs.cpu()\n",
    "    masks = masks.cpu()\n",
    "\n",
    "    #i枚举sentence里的个数，一个reference+一个candidate就是从0到1\n",
    "\n",
    "    for i, sen in enumerate(sen_batch):\n",
    "        sequence_len = masks[i].sum().item()\n",
    "        \n",
    "        print(\"sequence_len的值是：\")\n",
    "        print(sequence_len)\n",
    "\n",
    "        # For special tokens, use [SOS] and [EOS]\n",
    "        local_emb = embs[i, 0:sequence_len]\n",
    "        global_emb = embs[i, sequence_len-1]\n",
    "        idf = text_idfs[i, 0:sequence_len]\n",
    "\n",
    "        print(\"此处获得local embedding向量：\")\n",
    "        print(local_emb)\n",
    "        print(local_emb.shape)\n",
    "        print(\" 此处获得global embedding向量：\")\n",
    "        print(global_emb)\n",
    "        print(global_emb.shape)\n",
    "        print(\"获得的idf值：\")\n",
    "        print(idf)\n",
    "        print(idf.shape)\n",
    "\n",
    "        # For special tokens, don't use any\n",
    "        # local_emb = embs[i, 1:sequence_len+1]\n",
    "        # global_emb = embs[i, sequence_len+1]\n",
    "        # idf = text_idfs[i, 1:sequence_len+1]\n",
    "\n",
    "        # For special tokens, only use [EOS] \n",
    "        # local_emb = embs[i, 1:sequence_len+1]\n",
    "        # global_emb = embs[i, sequence_len]\n",
    "        # idf = text_idfs[i, 1:sequence_len+1]\n",
    "\n",
    "        text_local_stats_dict[sen] = (local_emb, idf)\n",
    "        text_global_stats_dict[sen] = global_emb\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_code=[]\n",
    "ref_code.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing code embedding.\n",
      "sequence_len的值是：\n",
      "24\n",
      "此处获得local embedding向量：\n",
      "tensor([[-0.0846, -0.0142,  0.0101,  ..., -0.0234,  0.0021, -0.0564],\n",
      "        [-0.0846, -0.0142,  0.0101,  ..., -0.0234,  0.0021, -0.0564],\n",
      "        [-0.0846, -0.0142,  0.0101,  ..., -0.0234,  0.0021, -0.0564],\n",
      "        ...,\n",
      "        [-0.0210,  0.0374, -0.0201,  ..., -0.0516,  0.0178, -0.0334],\n",
      "        [-0.0248,  0.0724, -0.0183,  ..., -0.0430,  0.0414, -0.0561],\n",
      "        [-0.0397,  0.0296, -0.0115,  ..., -0.0814,  0.0539, -0.0125]])\n",
      "torch.Size([24, 768])\n",
      " 此处获得global embedding向量：\n",
      "tensor([-3.9688e-02,  2.9626e-02, -1.1544e-02,  1.4652e-02,  5.2433e-02,\n",
      "         8.3955e-02, -2.3682e-02, -3.1725e-02, -2.5731e-02, -3.5622e-03,\n",
      "        -2.6908e-02, -3.0190e-02, -1.0007e-02, -3.6842e-02,  3.6083e-03,\n",
      "         1.0192e-02,  6.4037e-02,  2.9288e-02,  3.7607e-02,  4.2263e-02,\n",
      "        -1.0714e-02,  5.0626e-02,  7.7406e-03, -6.1101e-02,  7.4759e-02,\n",
      "         2.9475e-02,  4.8208e-02,  1.1013e-02,  2.3555e-02,  7.5120e-03,\n",
      "         5.5236e-02, -8.6937e-02,  3.4395e-02, -1.1472e-02, -2.1902e-03,\n",
      "        -2.0793e-02,  2.2604e-02, -2.6463e-02, -2.5392e-02, -2.7413e-02,\n",
      "        -1.6059e-02, -1.8257e-02, -3.3347e-02, -3.0615e-02, -4.8817e-03,\n",
      "        -8.9534e-02,  1.8637e-02,  4.0834e-03,  1.6255e-02,  1.8906e-02,\n",
      "        -8.2619e-03,  4.2363e-02, -2.7173e-02, -4.8429e-02, -8.3697e-03,\n",
      "         2.8979e-02, -4.2301e-03, -8.2980e-02,  1.5227e-02,  1.5467e-02,\n",
      "         3.0530e-02, -4.4914e-02,  3.0751e-02,  1.6315e-02,  8.6616e-03,\n",
      "         1.6505e-02, -3.4936e-02, -3.1935e-02,  1.2552e-02, -1.3459e-03,\n",
      "         2.0468e-02, -1.6583e-02,  5.5653e-02, -4.8486e-02, -1.5294e-02,\n",
      "        -2.9353e-02,  4.9815e-02, -2.4978e-02,  1.2670e-02,  5.0070e-02,\n",
      "         7.8162e-02,  1.1864e-02, -5.3739e-02, -3.5533e-02, -3.4983e-02,\n",
      "         1.3530e-02,  1.1670e-02,  7.2484e-03, -4.9168e-02,  4.6841e-02,\n",
      "        -2.1371e-02,  6.7775e-03, -3.4692e-03, -2.2660e-02, -2.0117e-02,\n",
      "         8.5635e-03,  2.6340e-02,  1.6155e-02, -5.0352e-02, -3.3443e-02,\n",
      "         2.6034e-02, -1.2680e-02, -1.0029e-02,  2.7730e-02, -6.0474e-03,\n",
      "        -1.0898e-02, -1.8782e-02,  1.2232e-02,  1.2710e-02,  9.7225e-03,\n",
      "        -4.7648e-03, -3.3728e-02,  4.8759e-02,  1.9220e-02, -5.7630e-02,\n",
      "         1.9602e-02, -6.1819e-03,  1.6081e-03,  2.8723e-02, -1.7947e-02,\n",
      "         3.7285e-02,  2.8682e-02,  5.1643e-02, -7.9991e-03,  5.9015e-03,\n",
      "        -1.0538e-03,  1.8018e-02, -4.2279e-02, -4.9129e-02, -3.9180e-02,\n",
      "        -3.3619e-02, -3.8516e-02, -3.5540e-02, -1.9006e-02,  5.1961e-02,\n",
      "        -1.7060e-03,  4.3371e-02, -1.6438e-02,  2.4036e-02,  4.6729e-02,\n",
      "        -2.1113e-03,  6.2323e-02,  2.2171e-02,  7.0578e-02, -6.7664e-03,\n",
      "         5.6773e-02,  1.5515e-02, -3.0295e-02,  1.6831e-02, -1.4188e-01,\n",
      "        -2.1535e-02,  9.1780e-04,  3.8427e-02, -5.7795e-02,  4.3317e-03,\n",
      "         2.2047e-02, -7.7779e-03, -4.1852e-02,  5.1712e-03,  2.7055e-02,\n",
      "         5.6848e-03, -5.4394e-02,  1.1889e-02,  4.5128e-02,  1.5501e-02,\n",
      "         3.6393e-02, -1.1106e-02, -3.1060e-02,  5.0151e-03,  7.1600e-02,\n",
      "        -4.9207e-02,  2.5079e-02, -3.0848e-03, -5.1004e-02, -3.6332e-03,\n",
      "         9.5216e-03, -1.8681e-02, -2.4420e-02,  4.3766e-02, -1.9698e-02,\n",
      "        -2.7191e-02, -1.2375e-03, -1.0085e-03,  6.0973e-03, -1.8638e-02,\n",
      "         2.6712e-03, -1.1671e-02, -1.0600e-02,  5.5008e-02, -1.5877e-02,\n",
      "        -3.2991e-02,  1.4077e-02,  4.2098e-02, -3.9001e-02, -1.4202e-03,\n",
      "        -3.3422e-02,  3.5612e-03,  1.2856e-01,  5.0852e-02, -2.6161e-02,\n",
      "         2.1730e-02, -3.8758e-02, -4.8508e-03, -5.7473e-02, -5.1275e-02,\n",
      "        -6.9259e-03, -3.6456e-02, -2.5303e-02,  5.1426e-02, -3.2599e-02,\n",
      "         2.2595e-02,  6.0540e-02,  3.0008e-02, -1.0040e-02, -2.1872e-03,\n",
      "         6.3513e-03,  1.3994e-02, -2.0096e-02,  6.7891e-03, -1.7537e-02,\n",
      "        -3.4658e-02, -1.8870e-02,  4.0383e-02,  3.1892e-02, -2.0037e-02,\n",
      "        -9.5422e-02, -1.7282e-02,  5.4292e-02,  7.0330e-02,  2.2669e-02,\n",
      "         2.5788e-02, -1.3855e-02,  5.2601e-02, -2.5825e-02,  9.0908e-03,\n",
      "         8.1265e-02, -3.2483e-02,  3.4589e-02, -3.5025e-02,  1.8174e-02,\n",
      "         1.2878e-03,  8.4933e-03, -3.1507e-02,  1.7207e-02,  2.9714e-02,\n",
      "        -6.0677e-02, -3.4156e-02,  4.5000e-02,  1.0972e-01,  7.4911e-03,\n",
      "         1.5247e-02,  4.0467e-02, -1.7488e-02, -3.5205e-02,  5.3527e-03,\n",
      "        -1.2169e-02,  3.9633e-03,  1.4094e-02, -4.1076e-02,  2.8901e-02,\n",
      "        -2.0268e-02, -1.7738e-03, -3.7066e-02, -9.2238e-04,  5.5417e-02,\n",
      "         2.0186e-02, -6.2376e-02, -3.3988e-02, -6.0378e-02, -5.9597e-02,\n",
      "         3.6433e-02,  5.6314e-02,  4.3845e-03,  1.5015e-02,  3.2179e-02,\n",
      "        -7.4306e-03,  2.1096e-02, -2.6129e-03,  3.0539e-02,  8.2498e-03,\n",
      "        -1.3711e-02, -1.1106e-02,  6.3528e-03, -4.2332e-02,  4.3544e-02,\n",
      "        -3.6083e-02,  3.7112e-02, -1.0580e-04, -3.7571e-02,  9.4757e-03,\n",
      "        -2.5959e-02,  8.1498e-02, -7.5165e-03,  8.8758e-03, -1.0052e-02,\n",
      "        -6.4753e-02, -1.1100e-02,  2.5162e-02, -1.1728e-02, -7.0119e-02,\n",
      "         7.3451e-02, -3.8846e-02,  1.0976e-02, -7.7983e-04, -1.7180e-02,\n",
      "         2.5501e-02, -1.8229e-02, -2.7707e-02, -1.5678e-02, -4.0061e-02,\n",
      "        -4.8283e-02, -1.0664e-03, -2.4660e-02,  1.9984e-02,  2.7601e-04,\n",
      "         6.7380e-02, -4.6375e-02,  3.3615e-02, -4.8508e-02, -1.7628e-02,\n",
      "        -1.6439e-02,  4.8301e-02, -1.9875e-02, -1.3180e-03,  4.7324e-02,\n",
      "        -8.9890e-03,  1.9495e-02, -7.4571e-02,  3.2827e-02,  7.2903e-04,\n",
      "         3.4509e-02, -4.6505e-02,  1.0149e-01,  2.6351e-03, -1.5405e-02,\n",
      "         2.4475e-02,  2.3059e-02,  2.9329e-02, -1.2359e-02,  2.9167e-02,\n",
      "         2.5193e-03,  3.4667e-02, -4.8587e-02, -1.3597e-03, -1.2420e-02,\n",
      "        -2.3611e-02, -1.7808e-02,  5.9673e-02, -4.2043e-02, -3.3264e-02,\n",
      "         6.7168e-03, -4.1658e-02,  2.2094e-04,  1.2031e-02, -4.3609e-02,\n",
      "         1.6612e-02,  6.9013e-03, -3.2108e-02, -1.8890e-02, -1.7306e-03,\n",
      "         2.2006e-03, -4.3388e-02, -1.9823e-02,  2.6856e-02, -2.9466e-03,\n",
      "        -2.7063e-02,  1.1451e-02, -3.4576e-03,  2.2509e-02, -5.3711e-03,\n",
      "        -2.5292e-02,  5.4223e-02,  3.4671e-03, -8.2233e-02, -2.3835e-02,\n",
      "        -1.6036e-02, -3.6211e-02, -6.3988e-03, -1.8311e-02,  1.1780e-02,\n",
      "         1.7778e-02, -8.7529e-03,  2.0793e-02, -9.9566e-02, -2.6495e-03,\n",
      "         4.6844e-02,  2.5550e-02, -2.9985e-02,  4.8566e-02, -4.2654e-02,\n",
      "        -1.2635e-02, -1.2106e-02, -4.2359e-02, -4.8447e-03,  2.9620e-02,\n",
      "        -4.0693e-03, -4.4076e-02, -2.2004e-02, -1.8657e-02,  6.2646e-02,\n",
      "        -7.5826e-03,  5.1934e-02,  3.2183e-02, -7.0146e-02, -5.5115e-02,\n",
      "         2.3127e-02, -6.8205e-02,  1.1164e-01, -4.5159e-02, -2.0362e-02,\n",
      "        -3.1354e-02, -1.9106e-02,  3.0138e-02,  1.0911e-02, -5.4186e-03,\n",
      "         3.8963e-02, -2.5941e-02, -5.6364e-02,  3.6484e-02,  2.3782e-02,\n",
      "         2.7125e-02,  4.3196e-02, -2.1685e-02, -1.3349e-03,  6.7245e-02,\n",
      "        -4.2241e-02,  1.5022e-02,  7.7127e-03,  2.9675e-02, -5.1591e-02,\n",
      "         3.2816e-02, -2.3423e-02, -4.2865e-02, -6.6941e-02,  5.8693e-02,\n",
      "        -2.7935e-03,  2.9530e-02, -1.7430e-02,  1.4306e-02, -3.7170e-03,\n",
      "         3.9994e-02,  3.0169e-03,  3.4645e-02, -1.1479e-02,  2.3985e-02,\n",
      "         7.2486e-02, -2.8034e-02,  3.7716e-02,  3.5372e-02,  3.7384e-05,\n",
      "        -5.0253e-03,  2.8396e-02, -2.5844e-02, -3.6598e-03, -1.4359e-02,\n",
      "        -2.4998e-02,  5.5565e-02, -1.6235e-02, -5.4006e-02,  1.0531e-02,\n",
      "        -1.3932e-02, -2.5869e-02,  4.7701e-02,  6.6108e-02, -2.3059e-02,\n",
      "         1.6164e-02,  3.3337e-02, -1.7768e-02,  4.0081e-02, -1.1924e-02,\n",
      "        -3.8936e-02, -9.7395e-03,  6.4568e-03, -3.1977e-02,  8.9560e-03,\n",
      "         4.7265e-02,  5.8561e-02, -1.9191e-02,  8.2642e-03, -1.7352e-02,\n",
      "         3.4500e-02, -5.3644e-03, -1.2194e-02,  4.9706e-04, -2.8303e-02,\n",
      "         8.9089e-03,  2.0989e-02, -1.9388e-02, -3.9753e-02,  2.5691e-02,\n",
      "        -5.3698e-02, -4.1550e-03,  7.7018e-04, -5.7440e-03,  2.2035e-02,\n",
      "         2.9104e-02,  2.5699e-02,  1.6920e-02,  9.6321e-03,  4.9142e-02,\n",
      "         1.5009e-02, -9.5308e-03,  1.5268e-02, -1.5652e-02,  1.5340e-02,\n",
      "        -4.7352e-02,  4.5270e-03, -4.2808e-02, -4.4726e-03, -1.7752e-02,\n",
      "         6.4284e-02, -2.8314e-02, -3.6836e-02,  5.9870e-02,  2.4510e-02,\n",
      "        -5.2276e-02, -1.1058e-02, -4.9713e-02,  3.1017e-02, -3.2169e-04,\n",
      "         4.7992e-02,  3.8279e-02,  2.1061e-02,  8.2539e-02,  5.5528e-02,\n",
      "        -1.7509e-02,  2.4566e-02, -3.5152e-03,  3.2979e-02,  1.4750e-02,\n",
      "         2.0264e-01,  3.2313e-03,  2.6812e-02,  1.1936e-02, -4.0326e-02,\n",
      "         7.1155e-02,  7.9682e-03, -5.7970e-02,  3.3840e-02,  1.9417e-02,\n",
      "         3.7159e-03,  2.8938e-02, -4.6765e-02,  5.0975e-02,  2.1132e-03,\n",
      "        -3.8846e-02,  1.7780e-02,  3.5466e-02,  1.2743e-02,  1.3334e-02,\n",
      "         1.7049e-02, -5.8215e-02,  3.6234e-02, -1.1285e-05,  3.8579e-03,\n",
      "         9.7094e-03, -4.6374e-02,  1.3103e-02, -3.4072e-03,  7.4293e-03,\n",
      "         2.7379e-04,  1.4341e-02,  8.7533e-02,  3.9365e-02, -3.3155e-03,\n",
      "         1.6941e-02, -1.2234e-02, -1.5794e-02, -8.6199e-02,  6.6477e-03,\n",
      "        -1.0779e-02, -6.8359e-02, -1.8914e-02, -4.1581e-02, -6.7798e-03,\n",
      "         2.7595e-02,  1.7703e-02,  7.9103e-02,  7.0346e-03,  2.1303e-02,\n",
      "         1.3760e-02,  3.0901e-03,  6.9956e-03, -1.8210e-02, -2.7308e-03,\n",
      "        -3.1157e-02,  4.0867e-02, -8.7082e-02, -2.6193e-02, -9.5221e-03,\n",
      "        -9.0753e-03, -1.2610e-02,  1.4095e-02, -2.5694e-02,  5.7316e-02,\n",
      "        -9.2736e-03,  6.6170e-02,  1.5994e-02, -3.7034e-02,  7.1068e-03,\n",
      "         2.8850e-02,  2.4679e-02, -1.6178e-03, -2.3516e-02,  4.2016e-02,\n",
      "        -4.2049e-03, -6.3820e-03, -2.8324e-03, -6.2349e-02,  2.0840e-02,\n",
      "         9.5998e-02, -1.1885e-02, -4.1917e-02, -2.9697e-02, -4.8839e-02,\n",
      "        -2.3861e-02,  1.0927e-02, -1.1341e-02, -3.5676e-02,  3.9975e-02,\n",
      "         8.0413e-02, -5.3889e-03,  5.1402e-02, -1.8420e-02,  4.0980e-02,\n",
      "         3.7740e-02,  6.1867e-04,  2.7924e-02,  1.7862e-02,  2.1072e-02,\n",
      "        -5.1608e-02, -1.4099e-02,  2.3280e-02,  3.7416e-02,  2.4701e-02,\n",
      "        -1.8045e-04,  2.0600e-02,  4.5706e-02, -7.1981e-04, -4.9354e-02,\n",
      "        -9.7181e-03,  6.1260e-03, -3.1751e-02, -7.1880e-03, -1.7747e-02,\n",
      "        -1.5793e-02,  2.1742e-02,  3.0870e-02,  5.8697e-03, -3.1160e-02,\n",
      "        -1.6969e-02,  1.7937e-02,  1.8416e-02, -1.2261e-02,  1.0622e-03,\n",
      "         2.9087e-02, -1.3222e-02,  1.5604e-02,  1.1744e-02,  6.0169e-02,\n",
      "        -1.9437e-02, -3.4973e-02, -1.6506e-02,  3.1885e-02,  2.4939e-02,\n",
      "         5.5252e-02,  7.9972e-03, -1.5287e-02,  3.3818e-02, -3.7280e-02,\n",
      "         4.5648e-03,  6.2175e-04, -8.4473e-02, -6.9198e-02, -5.5670e-02,\n",
      "        -3.0057e-02,  4.1096e-02, -5.1677e-02, -3.4281e-02,  3.0282e-02,\n",
      "         2.6040e-02,  1.4981e-02, -2.7345e-02, -6.1031e-02,  2.4426e-02,\n",
      "        -1.7047e-02,  3.3415e-02,  2.8201e-02,  4.4778e-02, -2.9771e-03,\n",
      "         2.0648e-02,  4.1982e-02,  2.5150e-02, -1.7423e-02,  5.7205e-03,\n",
      "         2.3636e-02,  2.0549e-02,  6.2399e-02, -6.0370e-02,  5.3978e-02,\n",
      "        -1.6775e-02, -3.1919e-02,  5.0699e-03, -9.3851e-03,  4.0960e-02,\n",
      "         4.3571e-02,  5.8668e-02,  3.6593e-02, -1.7562e-02,  1.2806e-02,\n",
      "         3.5819e-03, -3.5476e-02, -2.0468e-02, -2.2216e-02, -3.3015e-02,\n",
      "        -1.6607e-02, -6.6505e-03, -6.5452e-02,  1.6286e-02,  3.4837e-03,\n",
      "         5.5321e-04,  8.2083e-02,  6.8227e-02,  3.6165e-03, -4.2251e-02,\n",
      "        -4.9381e-02,  3.9655e-02, -5.6248e-05,  4.8589e-03, -4.7829e-02,\n",
      "        -4.5326e-02, -3.4821e-02, -1.5492e-02,  1.2341e-02, -2.2522e-02,\n",
      "         4.9874e-02,  2.6967e-03,  8.5266e-03,  3.1480e-02,  1.8063e-02,\n",
      "        -1.9189e-02, -9.8455e-03,  1.8295e-02, -1.8449e-02,  6.9707e-02,\n",
      "         5.7587e-02,  5.1808e-02, -7.6716e-02,  3.0687e-02, -8.3458e-04,\n",
      "         1.8858e-03, -5.5404e-04,  2.0585e-02, -1.7264e-02, -6.0913e-02,\n",
      "        -1.7631e-02,  6.5185e-03, -3.9034e-02, -4.9190e-02, -5.0230e-02,\n",
      "        -8.0240e-03, -6.2830e-02, -1.2576e-02, -4.4170e-02, -4.4956e-03,\n",
      "        -8.1405e-02,  5.3928e-02, -1.2461e-02])\n",
      "torch.Size([768])\n",
      "获得的idf值：\n",
      "tensor([0.0000, 0.0000, 5.0929, 0.0421, 5.4293, 0.1636, 3.6889, 0.1411, 0.0000,\n",
      "        0.0426, 4.3646, 0.0421, 3.5575, 0.2303, 4.3646, 0.0799, 1.2367, 0.0421,\n",
      "        1.5877, 0.0799, 5.7860, 3.5124, 0.0799, 5.1670])\n",
      "torch.Size([24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"process code\"\"\"\n",
    "code_sent = dedup_and_sort(ref_code)\n",
    "#sentences = dedup_and_sort(hyps)\n",
    "\n",
    "\n",
    "embs = []\n",
    "iter_range = range(0, len(code_sent), batch_size)\n",
    "if verbose:\n",
    "    print(\"computing code embedding.\")\n",
    "    iter_range = tqdm(iter_range)\n",
    "code_local_stats_dict = dict()\n",
    "code_global_stats_dict = dict()\n",
    "for batch_start in iter_range:\n",
    "    code_sen_batch = code_sent[batch_start: batch_start + batch_size]\n",
    "\n",
    "    #实际处理文本内容在这里调用encode_code\n",
    "    #此处将encode_code函数替换成处理代码任务中candidate的函数\n",
    "    code_embs, code_masks, code_idfs = encode_code(code_sen_batch[0], model, tokenizer, code_idf_dict, device=device)\n",
    "    code_embs = code_embs.cpu()\n",
    "    code_masks = code_masks.cpu()\n",
    "\n",
    "    #i枚举sentence里的个数，一个reference+一个candidate就是从0到1\n",
    "\n",
    "    for i, sen in enumerate(code_sen_batch):\n",
    "        sequence_len = code_masks[i].sum().item()\n",
    "        \n",
    "        print(\"sequence_len的值是：\")\n",
    "        print(sequence_len)\n",
    "\n",
    "        # For special tokens, use [SOS] and [EOS]\n",
    "        code_local_emb = code_embs[i, 0:sequence_len]\n",
    "        code_global_emb = code_embs[i, sequence_len-1]\n",
    "        code_idf = code_idfs[i, 0:sequence_len]\n",
    "\n",
    "        print(\"此处获得local embedding向量：\")\n",
    "        print(code_local_emb)\n",
    "        print(code_local_emb.shape)\n",
    "        print(\" 此处获得global embedding向量：\")\n",
    "        print(code_global_emb)\n",
    "        print(code_global_emb.shape)\n",
    "        print(\"获得的idf值：\")\n",
    "        print(code_idf)\n",
    "        print(code_idf.shape)\n",
    "\n",
    "        # For special tokens, don't use any\n",
    "        # local_emb = embs[i, 1:sequence_len+1]\n",
    "        # global_emb = embs[i, sequence_len+1]\n",
    "        # idf = text_idfs[i, 1:sequence_len+1]\n",
    "\n",
    "        # For special tokens, only use [EOS] \n",
    "        # local_emb = embs[i, 1:sequence_len+1]\n",
    "        # global_emb = embs[i, sequence_len]\n",
    "        # idf = text_idfs[i, 1:sequence_len+1]\n",
    "\n",
    "        text_local_stats_dict[sen] = (local_emb, idf)\n",
    "        text_global_stats_dict[sen] = global_emb\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_local_batch_stats(sen_batch, stats_dict, device):\n",
    "    stats = [stats_dict[s] for s in sen_batch]\n",
    "    emb, idf = zip(*stats)\n",
    "    emb = [e.to(device) for e in emb]\n",
    "    lens = [e.size(0) for e in emb]\n",
    "    emb_pad = pad_sequence(emb, batch_first=True, padding_value=0.0)\n",
    "    idf_pad = pad_sequence(idf, batch_first=True)\n",
    "\n",
    "    def length_to_mask(lens):\n",
    "        lens = torch.tensor(lens, dtype=torch.long)\n",
    "        max_len = max(lens)\n",
    "        base = torch.arange(max_len, dtype=torch.long).expand(len(lens), max_len)\n",
    "        return base < lens.unsqueeze(1)\n",
    "\n",
    "    pad_mask = length_to_mask(lens).to(device)\n",
    "    return emb_pad, pad_mask, idf_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vid_local_batch_stats(sen_batch, stats_dict, device):\n",
    "    stats = [stats_dict[s] for s in sen_batch]\n",
    "    emb = stats\n",
    "    emb = [e.to(device) for e in emb]\n",
    "    lens = [e.size(0) for e in emb]\n",
    "    emb_pad = pad_sequence(emb, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    def length_to_mask(lens):\n",
    "        lens = torch.tensor(lens, dtype=torch.long)\n",
    "        max_len = max(lens)\n",
    "        base = torch.arange(max_len, dtype=torch.long).expand(len(lens), max_len)\n",
    "        return base < lens.unsqueeze(1)\n",
    "\n",
    "    pad_mask = length_to_mask(lens).to(device)\n",
    "    return emb_pad, pad_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_global_batch_stats(sen_batch, stats_dict, device):\n",
    "    stats = [stats_dict[s] for s in sen_batch]\n",
    "    emb = stats\n",
    "    emb = [e.to(device) for e in emb]\n",
    "    emb_pad = pad_sequence(emb, batch_first=True, padding_value=0.0)\n",
    "    return emb_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分是用video作为ground truth计算匹配的分数，对比着这部分进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid_greedy_cos(ref_embedding, ref_masks, hyp_embedding, hyp_masks, hyp_idf, return_matched_idx):\n",
    "    \"\"\"\n",
    "    Compute greedy matching based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        - :param: `ref_embedding` (torch.Tensor):\n",
    "                   embeddings of reference sentences, BxKxd,\n",
    "                   B: batch size, K: longest length, d: bert dimenison.\n",
    "        - :param: `ref_masks` (torch.LongTensor): BxKxK, BERT attention mask for\n",
    "                   reference sentences.\n",
    "        - :param: `hyp_embedding` (torch.Tensor):\n",
    "                   embeddings of candidate sentences, BxKxd,\n",
    "                   B: batch size, K: longest length, d: bert dimenison\n",
    "        - :param: `hyp_masks` (torch.LongTensor): BxKxK, BERT attention mask for\n",
    "                   candidate sentences.\n",
    "    \"\"\"\n",
    "    # ref_embedding and hyp_embedding are aleady L2-normalized.\n",
    "\n",
    "    batch_size = ref_embedding.size(0)\n",
    "    sim = torch.bmm(hyp_embedding, ref_embedding.transpose(1, 2))\n",
    "    masks = torch.bmm(hyp_masks.unsqueeze(2).float(), ref_masks.unsqueeze(1).float())\n",
    "    masks = masks.expand(batch_size, -1, -1).contiguous().view_as(sim)\n",
    "    masks = masks.float().to(sim.device)\n",
    "    sim = sim * masks\n",
    "\n",
    "    word_precision, matched_indices = sim.max(dim=2)\n",
    "    word_recall = sim.max(dim=1)[0]\n",
    "\n",
    "    hyp_idf.div_(hyp_idf.sum(dim=1, keepdim=True))\n",
    "    precision_scale = hyp_idf.to(word_precision.device)\n",
    "    P = (word_precision * precision_scale).sum(dim=1)\n",
    "    R = word_recall.sum(dim=1)/ref_masks.sum(dim=1)\n",
    "    F = 2 * P * R / (P + R)\n",
    "    \n",
    "    if return_matched_idx:\n",
    "        return P, R, F, matched_indices\n",
    "    else:\n",
    "        return P, R, F, torch.zeros_like(P)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\" if video used as ground truth \"\"\"\n",
    "if vids:\n",
    "    if verbose:\n",
    "        print(\"computing greedy matching, video as ground truth.\")\n",
    "    iter_range = range(0, len(ori_cands), batch_size)    \n",
    "    with torch.no_grad():\n",
    "        for batch_start in iter_range: \n",
    "            batch_ori_hyp = ori_cands[batch_start: batch_start + batch_size]\n",
    "            ori_hyp_stats_local = pad_local_batch_stats(batch_ori_hyp, text_local_stats_dict, device)\n",
    "            ori_hyp_stats_global = pad_global_batch_stats(batch_ori_hyp, text_global_stats_dict, device)\n",
    "\n",
    "            batch_ori_vids = ori_vids[batch_start: batch_start + batch_size]\n",
    "            ori_vids_stats_local = pad_vid_local_batch_stats(batch_ori_vids, vid_local_stats_dict, device)\n",
    "            ori_vids_stats_global = pad_global_batch_stats(batch_ori_vids, vid_global_stats_dict, device)\n",
    "\n",
    "            #在此处计算分数\n",
    "            P, R, F1, matched_indices = vid_greedy_cos(*ori_vids_stats_local, *ori_hyp_stats_local, return_matched_idx)\n",
    "            vid_preds_local.append(torch.stack((P, R, F1), dim=-1).cpu())\n",
    "            vid_pred_matched_idxs.append(matched_indices)\n",
    "\n",
    "            vid_s_cogr = torch.bmm(ori_hyp_stats_global.unsqueeze(1), ori_vids_stats_global.unsqueeze(1).transpose(1, 2)).squeeze()\n",
    "            vid_preds_global.append(vid_s_cogr)  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "196005e07f823faab020780d1971fd9ef55151a4e2624f761e89102bc97b9bea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
